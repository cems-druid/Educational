{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "technological-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 7 0 2 4 0 2 4 1 4]\n",
      " [5 6 2 5 4 5 6 8 5 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The codes are gathered from \"The Machine Learning Workshop\" book from 230th page.\n",
    "import numpy as np\n",
    "import torch\n",
    "#It is important to feed the lstm numerical data appropriately. This needs a series of transformations.\n",
    "\n",
    "#Numerazing the input\n",
    "text = \"this is a test text!\"\n",
    "chars = list(set(text))\n",
    "\n",
    "indexer = {char:index for (index,char) in enumerate(chars)}\n",
    "indexer\n",
    "\n",
    "indexed_data = []\n",
    "for c in text:\n",
    "    indexed_data.append(indexer[c])\n",
    "    \n",
    "#preparing batches to feed reshape array into 2 first\n",
    "x = np.array(indexed_data).reshape((2,-1))\n",
    "#in this case batches are prepared for 4 row 5 column matrices\n",
    "for b in range(0, x.shape[1],5):\n",
    "    batch = x[:, b:b+5]\n",
    "    print(batch)\n",
    "\n",
    "    #one-hot encoding\n",
    "#an array\n",
    "batch1 = np.array([[2,4,7,6,5],\n",
    "                 [2,1,6,2,5]])\n",
    "#making the array 1d \"flattening\"\n",
    "batch1_flatten = batch1.flatten()\n",
    "print(batch1_flatten)\n",
    "\n",
    "#creating onehot matrix \n",
    "onehot_flat = np.zeros((batch1.shape[0] * batch1.shape[1], len(indexer)))\n",
    "print(onehot_flat, onehot_flat.shape)\n",
    "\n",
    "#putting 1s where the chars are matched.\n",
    "onehot_flat[range(len(batch1_flatten)), batch1_flatten] = 1\n",
    "print(onehot_flat)\n",
    "\n",
    "onehot = onehot_flat.reshape((batch1.shape[0], batch1.shape[1], -1))\n",
    "onehot\n",
    "\n",
    "#turn this into a function\n",
    "def index2onehot(batch):\n",
    "    batch_flatten = batch.flatten()\n",
    "    onehot_flat = np.zeros((batch.shape[0]*batch.shape[1], len(indexer)))\n",
    "    onehot_flat[range(len(batch_flatten)), batch_flatten] = 1\n",
    "    onehot = onehot_flat.reshape((batch.shape[0], batch.shape[1], -1))\n",
    "    \n",
    "    return onehot\n",
    "\n",
    "#an example lstm network architecture\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, char_length, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(char_length, hidden_size, n_layers, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, char_length)\n",
    "        \n",
    "    def forward(self, x, states):\n",
    "        out, states = self.lstm(x, states)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.output(out)\n",
    "        \n",
    "        return out, states\n",
    "    \n",
    "    def init_states(self, batch_size):\n",
    "        hidden = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
    "        cell = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
    "        states = (hidden, cell)\n",
    "        \n",
    "        return states\n",
    "    \n",
    "#Training the model\n",
    "for e in range(1, epochs+1):\n",
    "    #call to recreate hidden and cell states each epoch\n",
    "    states = model.init_states(n_seq)\n",
    "    \n",
    "    #Batching the data and feeding to network.\n",
    "    for b in range(0, x.shape[1], seq_length):\n",
    "        x_batch = x[:, b:b+seq_length]\n",
    "        \n",
    "        #check wheter it is the last batch and put a period if it is.\n",
    "        if b == x.shape[1]-seq_length:\n",
    "            y_batch = x[:, b+1:b+seq_length]\n",
    "            y_batch = np.hstack((y_batch, indexer[\".\"] * np.ones((y_batch.shape[0],1))))\n",
    "        else:\n",
    "            y_batch = x[:, b+1:b+seq_length+1]\n",
    "            \n",
    "        #input is changed into one-hot encoding and PyTorch tensors\n",
    "        x_onehot = torch.Tensor(index2onehot(x_batch))\n",
    "        y = torch.Tensor(y_batch).view(n_seq*seq_length)\n",
    "        \n",
    "        #call the model for each batch of data.\n",
    "        pred, states = model(x_onehot, states)\n",
    "        #Calculate the loss\n",
    "        loss = loss_function(pred, y.long())\n",
    "        #Optimize the parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "          \n",
    "#Performing predictions\n",
    "starter = \"This is the starter text\"\n",
    "states = None\n",
    "\n",
    "# feeding each character into model in order to update the memory of the model\n",
    "for ch in starter:\n",
    "    x = np.array([[indexer[ch]]])\n",
    "    x = index2onehot(x)\n",
    "    x = tensor.Torch(x)\n",
    "    pred, states = model(x, states)\n",
    "\n",
    "#perform a prediction until dot or character limit of 50\n",
    "counter = 0\n",
    "while starter[-1] != \".\" and counter<50:\n",
    "    x = np.array([[indexer[starter[-1]]]])\n",
    "    x = index2onehot(x)\n",
    "    x = torch.Tensor(x)\n",
    "    \n",
    "    pred, states = model(x, states)\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    p, top = pred.topk(10)\n",
    "    p = p.detach().numpy()[0]\n",
    "    top = top.numpy()[0]\n",
    "    index = np.random.choice(top, p=p/p.sum())\n",
    "    \n",
    "    starter += chars[index]\n",
    "    print(starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "honest-baltimore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10/5/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "decimal-ghana",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-3b0176f3d7e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "international-breath",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-0a783dcc758d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstarter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"This is the starter text\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'T'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
